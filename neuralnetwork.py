import math
import numpy as np

from instance import Instance
from utils import FileUtils


class NeuralNetwork(object):

    def __init__(self, initial_weights_file, neurons_per_layer, dataset_file):
        self.dataset_file = dataset_file
        self.initial_weights_file = initial_weights_file
        self.output     = []
        self.prediction = []
        self.dataset = None
        self.num_layers = len(neurons_per_layer)
        self.neurons_per_layer = neurons_per_layer
        self.training_data = None

        # thetha[L][i][j]: L camada, i neur√¥nio da camada seguinte, j neur√¥nio camada atual
        # thetha[L][i][j] √© o peso conectando o neur√¥nio j da camada L ao neur√¥nio i da camada seguinte
        max_neurons_per_layer = max(self.neurons_per_layer) + 1
        self.theta = [[[0 for l in range(self.num_layers)] for i in range(max_neurons_per_layer)] for j in range(max_neurons_per_layer)]
        self.initTheta()

        # activation[L][j] = ativa√ß√£o/sa√≠da do neur√¥nio j da camada L
        self.activation = [[0 for l in range(self.num_layers)] for j in range(max_neurons_per_layer)]

        self.setDataset()
        self.setTrainingData()

    def initTheta(self):
        with open(self.initial_weights_file) as f:
            data = f.readlines()
            layer_index = 0

            for line in data:
                line = line.strip('\n')
                weights = line.split(';')

                for neuronI in range(len(weights)):
                    # i = neur√¥nio i da camada seguinte
                    i_weights = weights[neuronI].split(',')

                    for neuronJ in range(len(i_weights)):
                        # neuronJ = neur√¥nio neuronJ da camada atual
                        self.theta[layer_index][neuronI][neuronJ] = float(i_weights[neuronJ])
                layer_index = layer_index + 1


    def setDataset(self):
        fileUtils = FileUtils(self.dataset_file)
        self.dataset = fileUtils.getDataset()

    def setTrainingData(self):
        self.training_data = self.dataset

    def g(self, x):
        return float(1/(1 + math.exp(-x)))


    def backpropagateData(self):
        ex1 = Instance(attributes=[0.13], classification=[0.9])
        ex2 = Instance(attributes=[0.42], classification=[0.23])
        training_data = [ex1, ex2]
        delta = []

        for inst in training_data:
            pred = self.backpropagation(inst)


    def backpropagation(self, instance):
        print('--> backpropagation para instancia: (x = {0}, y = {1})'.format(instance.attributes, instance.classification))
        # z √© um vetor de tamanho num_layers
        z = [0 for i in range(self.num_layers)]
        bias = [1]

        self.activation[0] = bias + instance.attributes

        # Para cada camada k=2 (iniciando contagem em 1) at√© k=num_layers-1
        for k in range(1, self.num_layers-2):
            # Ativa√ß√£o dos neur√¥nios da camada k
            # z[k] = self.theta[k-1] * self.activation[k-1]
            z[k] = np.dot(self.theta[k-1], self.activation[k-1])
            a = self.g(z[k])
            self.activation[k] = bias + a

        # Ativa√ß√£o do neur√¥nio da camada de sa√≠da
        k = self.num_layers-1

        z[k] = np.dot(self.theta[self.num_layers-2], self.activation[self.num_layers-2])

        g_vector_function = np.vectorize(self.g)
        self.activation[k] = g_vector_function(z[k])

        print('Vetor a = {0}'.format(self.activation))
        print('Vetor z = {0}'.format(z))
        print('Vetor de theta = {0}'.format(self.theta))
        print('Predicao para k={0} = {1}'.format(k, self.activation[k]))

        # Predi√ß√£o final
        return self.activation[k]


    def calculateError(self, f_x, y):
        # J(i) = -y(i) .* log(fŒ∏(x(i))) - (1-y(i)) .* log(1 - fŒ∏(x(i)))
        error = float(-y )


    def networkCostFunction(self):
        J = 0 # J acumula o erro total da rede

        for i in range(len(training_data)):
            J = self.costFunction(training_data[i])

        # retorna o custo total da rede
        return J


    def costFunction(self, instance, J, num_examples):
        prediction = self.backpropagation(instance)

        # Calcula o vetor J(i) com o custo associado √† cada sa√≠da da rede
        # para o exemplo atual
        prediction_J = []
        for i in range(len(prediction)):
            prediction_J[i] = self.calculateError(f_x, instance.classification)
            J  = J + sum(prediction_J)

            # Divide o erro total calculado pelo n√∫mero de exemplos
            J = float(J/num_examples)

            # eleva cada peso da rede ao quadrado (exceto os pesos de bias) e os soma
            S = (Œª/(2 * num_examples)) * S

        # Retorna o custo regularizado J+S
        return J+S


    # TODO: executa esse processo at√© o criterio de parada -- definir crit√©rio
    def mainAlgorithm(self):
        ex1 = Instance(attributes=[0.13], classification=[0.9])
        ex2 = Instance(attributes=[0.42], classification=[0.23])
        training_data = [ex1, ex2]
        delta = []

        for inst in training_data:
            f_x = self.backpropagation(inst)
            error = float(f_x - inst.classification)
            delta.append(error)

            # calcula os deltas para as camadas ocultas
            for k in range(1, self.num_layers-1):
                # Remove o primeiro elemento de ùõø(l=k) (i.e., o delta associado ao neuroÃÇnio de bias da camada k)
                # ùõø(l=k) = [Œ∏(l=k)]T ùõø(l=k+1) .* a(l=k) .* (1-a(l=k))
                pass
            # Para cada camada k=L-1...1
            for k in range(self.num_layers-1, 0):
                # atualiza os gradientes dos pesos de cada camada com base no exemplo atual
                # acumula em D(l=k) os gradientes com base no exemplo atual
                # D(l=k) = D(l=k) + ùõø(l=k+1) [a(l=k)]T
                pass

        # Calcula gradientes finais (regularizados) para os pesos de cada camada
        for k in range(self.num_layers-1, 0):
            # Seja P(l=k) igual aÃÄ (Œª .* Œ∏(l=k)), mas com a primeira coluna zerada // aplica regularizacÃßaÃÉo Œª apenas a pesos naÃÉo bias
            # D(l=k) = (1/n) (D(l=k) + P(l=k)) // combina gradientes com regularizacÃßaÃÉo; divide por #exemplos para calcular gradiente meÃÅdio
            pass
        #3. // Nesse ponto, D(l=1) conteÃÅm os gradientes dos pesos em Œ∏(l=1); ...; D(l=L-1) conteÃÅm os gradientes dos pesos em Œ∏(l=L-1)

        # atualiza pesos de cada camada com base nos gradientes
        for k in range(self.num_layers-1, 0):
            # Œ∏(l=k) = Œ∏(l=k) - Œ± .* D(l=k)
            pass
